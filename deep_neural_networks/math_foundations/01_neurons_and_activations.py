"""
ğŸ§  DEEP NEURAL NETWORKS â€” Module 1: Neurons and Activation Functions
======================================================================

Learning Objectives:
  1. Understand the biological neuron â†’ mathematical neuron analogy
  2. Master the neuron equation: z = wÂ·x + b, output = activation(z)
  3. Know 6 activation functions: Step, Sigmoid, ReLU, Leaky ReLU, Tanh, Softmax
  4. Understand WHY activation functions matter: non-linearity
  5. Recognize the vanishing gradient problem in sigmoid/tanh
  6. Know the dying ReLU problem and when to use Leaky ReLU
  7. Compute a neuron output by hand

YouTube Resources:
  â­ 3Blue1Brown - But what is a neural network? https://www.youtube.com/watch?v=aircAruvnKk
  â­ StatQuest - Neural Networks Pt.1 https://www.youtube.com/watch?v=CqOfi41LfDw
  ğŸ“š Andrej Karpathy - The spelled-out intro to neural networks https://www.youtube.com/watch?v=VMj-3S1tku0

Time Estimate: 45-60 minutes
Difficulty: Beginner
Prerequisites: Linear algebra basics (dot product), basic Python
Key Concepts: neuron, weight, bias, activation function, non-linearity
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import os

# Create visuals directory
VIS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "visuals", "01_neurons_activations")
os.makedirs(VIS_DIR, exist_ok=True)


print("=" * 70)
print("ğŸ§  MODULE 1: NEURONS AND ACTIVATION FUNCTIONS")
print("=" * 70)
print()
print("Welcome to Deep Neural Networks!")
print("This module is about the SMALLEST unit: a single artificial neuron.")
print()


# ======================================================================
# SECTION 1: The Biological Inspiration
# ======================================================================
print("=" * 70)
print("SECTION 1: THE BIOLOGICAL INSPIRATION")
print("=" * 70)
print()
print("Your brain has ~86 BILLION neurons. Each one:")
print("  ğŸ”µ Receives electrical signals from other neurons (inputs)")
print("  ğŸ”µ Combines those signals (weighted sum)")
print("  ğŸ”µ Decides whether to 'fire' â€” send a signal forward (activation)")
print()
print("An artificial neuron copies this exact structure:")
print()
print("  Biological neuron    â†’    Artificial neuron")
print("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print("  Dendrites (inputs)   â†’    x1, x2, ..., xn")
print("  Synapse strength     â†’    weights w1, w2, ..., wn")
print("  Cell body (sum)      â†’    z = wÂ·x + b")
print("  Axon (firing)        â†’    output = activation(z)")
print()


# ======================================================================
# SECTION 2: The Neuron Equation
# ======================================================================
print("=" * 70)
print("SECTION 2: THE NEURON EQUATION")
print("=" * 70)
print()
print("A neuron takes n inputs and produces 1 output:")
print()
print("  STEP 1 â€” Weighted sum:")
print("  z = w1*x1 + w2*x2 + ... + wn*xn + b")
print("  (This is a dot product: z = w Â· x + b)")
print()
print("  STEP 2 â€” Apply activation function:")
print("  output = activation(z)")
print()
print("The WEIGHTS (w) control how important each input is.")
print("The BIAS (b) shifts the activation threshold.")
print()

# Manual example
print("--- HAND CALCULATION EXAMPLE ---")
print()
print("Imagine a neuron deciding 'Will it rain today?'")
print()
x = np.array([0.8, 0.3, 0.9])   # humidity, wind speed, cloud cover (0-1 scale)
w = np.array([0.6, 0.2, 0.8])   # weights (importance)
b = -0.5                          # bias

feature_names = ["Humidity", "Wind Speed", "Cloud Cover"]
for i, (xi, wi, fname) in enumerate(zip(x, w, feature_names)):
    print(f"  {fname}: input={xi}, weight={wi} â†’ contribution={xi*wi:.3f}")

z = np.dot(w, x) + b
print()
print(f"  Weighted sum: z = {w[0]}Ã—{x[0]} + {w[1]}Ã—{x[1]} + {w[2]}Ã—{x[2]} + ({b})")
print(f"  z = {w[0]*x[0]:.3f} + {w[1]*x[1]:.3f} + {w[2]*x[2]:.3f} + ({b})")
print(f"  z = {z:.3f}")
print()
print(f"  Before activation: z = {z:.3f}")
print(f"  Sigmoid(z) = 1/(1+e^(-{z:.3f})) = {1/(1+np.exp(-z)):.3f}")
print(f"  â†’ ~{1/(1+np.exp(-z))*100:.0f}% probability of rain")
print()


# ======================================================================
# SECTION 3: Why Activation Functions Matter
# ======================================================================
print("=" * 70)
print("SECTION 3: WHY ACTIVATION FUNCTIONS MATTER")
print("=" * 70)
print()
print("Without activation functions, neural networks can ONLY learn")
print("linear relationships â€” no matter how many layers you add!")
print()
print("  Linear: y = w2*(w1*x + b1) + b2 = (w1*w2)*x + (w2*b1+b2)")
print("  â†‘ Still just a line! Extra layers do nothing.")
print()
print("Activation functions introduce NON-LINEARITY.")
print("Non-linearity lets networks learn curves, spirals, complex boundaries!")
print()
print("Think of it like this:")
print("  Linear model â†’ can only draw a straight line through data")
print("  Neural net with activations â†’ can draw ANY shape")
print()


# ======================================================================
# SECTION 4: The 6 Activation Functions
# ======================================================================
print("=" * 70)
print("SECTION 4: THE 6 ACTIVATION FUNCTIONS")
print("=" * 70)
print()

x_range = np.linspace(-5, 5, 500)

def step_function(z):
    return (z >= 0).astype(float)

def sigmoid(z):
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

def relu(z):
    return np.maximum(0, z)

def leaky_relu(z, alpha=0.01):
    return np.where(z > 0, z, alpha * z)

def tanh_fn(z):
    return np.tanh(z)

def softmax(z):
    e = np.exp(z - np.max(z))
    return e / e.sum()


# ---- Step Function ----
print("1. STEP FUNCTION (historical, rarely used now)")
print("   â€¢ Output: 0 if z<0, 1 if zâ‰¥0")
print("   â€¢ Problem: gradient is 0 everywhere â†’ can't learn with backprop!")
print("   â€¢ Used in: the original 1957 perceptron")
print()

# ---- Sigmoid ----
print("2. SIGMOID Ïƒ(z) = 1 / (1 + e^(-z))")
print("   â€¢ Output range: (0, 1) â†’ great for probabilities")
print("   â€¢ Problem: VANISHING GRADIENT near z=-5 or z=5")
print("     (gradient â‰ˆ 0 at extremes â†’ learning stops)")
print("   â€¢ Used in: output layer for binary classification")
print()

# ---- ReLU ----
print("3. ReLU â€” Rectified Linear Unit: max(0, z)")
print("   â€¢ Output: 0 if z<0, z if zâ‰¥0")
print("   â€¢ Fast to compute, no vanishing gradient for z>0")
print("   â€¢ Problem: DYING ReLU â€” if z<0 always, neuron never activates")
print("   â€¢ Used in: hidden layers (the DEFAULT choice)")
print()

# ---- Leaky ReLU ----
print("4. LEAKY ReLU: max(0.01z, z)")
print("   â€¢ Like ReLU but small slope for z<0 (fixes dying ReLU)")
print("   â€¢ Used in: hidden layers when dying ReLU is a problem")
print()

# ---- Tanh ----
print("5. TANH: (e^z - e^(-z)) / (e^z + e^(-z))")
print("   â€¢ Output range: (-1, 1) â†’ zero-centered (helpful for training)")
print("   â€¢ Still has vanishing gradient at extremes (less severe than sigmoid)")
print("   â€¢ Used in: hidden layers (older networks), RNNs")
print()

# ---- Softmax ----
print("6. SOFTMAX: e^zi / Î£(e^zj)")
print("   â€¢ Converts a vector of scores â†’ probability distribution (sums to 1)")
print("   â€¢ Used in: OUTPUT layer for multi-class classification")
scores = np.array([2.0, 1.0, 0.1])
probs = softmax(scores)
print(f"   â€¢ Example: scores [2.0, 1.0, 0.1] â†’ probabilities {probs.round(3)}")
print(f"     Sum: {probs.sum():.3f} âœ…")
print()


# ======================================================================
# SECTION 5: Derivatives (Why They Matter for Learning)
# ======================================================================
print("=" * 70)
print("SECTION 5: DERIVATIVES â€” WHY THEY MATTER FOR LEARNING")
print("=" * 70)
print()
print("During training, we need to compute GRADIENTS (derivatives)")
print("of the loss with respect to the weights.")
print()
print("The activation function's derivative appears in EVERY gradient calculation.")
print()
print("If the derivative is near ZERO â†’ gradient is near ZERO â†’ weights don't update")
print("This is the VANISHING GRADIENT PROBLEM.")
print()

# Compute derivatives
eps = 1e-5
sigmoid_vals = sigmoid(x_range)
sigmoid_deriv = sigmoid_vals * (1 - sigmoid_vals)   # analytical derivative
tanh_vals = tanh_fn(x_range)
tanh_deriv = 1 - tanh_vals**2

relu_deriv = (x_range > 0).astype(float)

print("Maximum derivatives:")
print(f"  Sigmoid: max derivative = {sigmoid_deriv.max():.3f} (at z=0)")
print(f"  Tanh:    max derivative = {tanh_deriv.max():.3f} (at z=0)")
print(f"  ReLU:    derivative = 1.0 for all z>0 âœ… (no vanishing!)")
print()
print("â†’ ReLU has the best gradient properties for deep networks")
print()


# ======================================================================
# SECTION 6: VISUALIZATIONS
# ======================================================================
print("=" * 70)
print("SECTION 6: VISUALIZATIONS")
print("=" * 70)
print()

# --- PLOT 1: All 6 activation functions ---
print("ğŸ“Š Generating: All 6 activation functions...")

activations = {
    "Step Function": (step_function(x_range), "royalblue", "z >= 0 ? 1 : 0"),
    "Sigmoid Ïƒ(z)": (sigmoid(x_range), "darkorange", "1/(1+e^-z)"),
    "ReLU": (relu(x_range), "green", "max(0, z)"),
    "Leaky ReLU": (leaky_relu(x_range), "red", "max(0.01z, z)"),
    "Tanh": (tanh_fn(x_range), "purple", "(e^z - e^-z)/(e^z + e^-z)"),
    "Softmax*": (sigmoid(x_range), "brown", "e^z / Î£e^z  (*single input)"),
}

fig, axes = plt.subplots(2, 3, figsize=(15, 9))
fig.suptitle("ğŸ§  Activation Functions: The Non-Linear Magic", fontsize=16, fontweight="bold")
axes = axes.flatten()

for ax, (name, (vals, color, formula)) in zip(axes, activations.items()):
    ax.plot(x_range, vals, color=color, linewidth=2.5)
    ax.axhline(0, color="gray", linewidth=0.8, linestyle="--")
    ax.axvline(0, color="gray", linewidth=0.8, linestyle="--")
    ax.set_title(f"{name}", fontsize=13, fontweight="bold")
    ax.set_xlabel("z (pre-activation)", fontsize=10)
    ax.set_ylabel("output", fontsize=10)
    ax.text(0.05, 0.95, formula, transform=ax.transAxes, fontsize=9,
            verticalalignment="top", bbox=dict(boxstyle="round", facecolor="lightyellow", alpha=0.8))
    ax.grid(True, alpha=0.3)
    ax.set_xlim(-5, 5)

plt.tight_layout()
plt.savefig(os.path.join(VIS_DIR, "activation_functions.png"), dpi=300, bbox_inches="tight")
plt.close()
print("   âœ… Saved: activation_functions.png")


# --- PLOT 2: Neuron diagram ---
print("ğŸ“Š Generating: Neuron diagram...")

fig, ax = plt.subplots(1, 1, figsize=(12, 7))
ax.set_xlim(0, 10)
ax.set_ylim(0, 8)
ax.axis("off")
ax.set_facecolor("#f8f9fa")
fig.patch.set_facecolor("#f8f9fa")

ax.set_title("ğŸ§  The Artificial Neuron", fontsize=16, fontweight="bold", pad=15)

# Input nodes
inputs = [(1.5, 6.5), (1.5, 4.0), (1.5, 1.5)]
input_labels = ["xâ‚ = 0.8\n(Humidity)", "xâ‚‚ = 0.3\n(Wind)", "xâ‚ƒ = 0.9\n(Clouds)"]
weights = ["wâ‚=0.6", "wâ‚‚=0.2", "wâ‚ƒ=0.8"]
colors_input = ["#4CAF50", "#2196F3", "#FF9800"]

for (ix, iy), label, color in zip(inputs, input_labels, colors_input):
    circ = plt.Circle((ix, iy), 0.5, color=color, zorder=3, alpha=0.8)
    ax.add_patch(circ)
    ax.text(ix, iy, label, ha="center", va="center", fontsize=7.5,
            fontweight="bold", color="white", zorder=4)

# Neuron (summation node)
neuron_x, neuron_y = 5.5, 4.0
circ = plt.Circle((neuron_x, neuron_y), 0.8, color="#9C27B0", zorder=3, alpha=0.9)
ax.add_patch(circ)
ax.text(neuron_x, neuron_y + 0.15, "Î£", ha="center", va="center",
        fontsize=20, color="white", zorder=4, fontweight="bold")
ax.text(neuron_x, neuron_y - 0.45, "z = wÂ·x+b", ha="center", va="center",
        fontsize=7, color="white", zorder=4)

# Bias node
ax.add_patch(plt.Circle((3.5, 0.5), 0.4, color="#607D8B", zorder=3, alpha=0.8))
ax.text(3.5, 0.5, "b=-0.5\n(bias)", ha="center", va="center",
        fontsize=7, color="white", zorder=4, fontweight="bold")
ax.annotate("", xy=(neuron_x - 0.8, neuron_y - 0.3), xytext=(3.9, 0.7),
            arrowprops=dict(arrowstyle="->", color="#607D8B", lw=1.5))

# Activation node
act_x, act_y = 8.0, 4.0
ax.add_patch(plt.Circle((act_x, act_y), 0.7, color="#E91E63", zorder=3, alpha=0.9))
ax.text(act_x, act_y + 0.15, "Ïƒ", ha="center", va="center",
        fontsize=18, color="white", zorder=4, fontweight="bold")
ax.text(act_x, act_y - 0.4, "sigmoid", ha="center", va="center",
        fontsize=7, color="white", zorder=4)

# Arrows from inputs to neuron
for (ix, iy), weight, color in zip(inputs, weights, colors_input):
    ax.annotate("", xy=(neuron_x - 0.8, neuron_y), xytext=(ix + 0.5, iy),
                arrowprops=dict(arrowstyle="->", color=color, lw=2.0))
    mid_x = (ix + neuron_x) / 2
    mid_y = (iy + neuron_y) / 2
    ax.text(mid_x, mid_y + 0.3, weight, ha="center", fontsize=9,
            color=color, fontweight="bold",
            bbox=dict(boxstyle="round,pad=0.2", facecolor="white", edgecolor=color, alpha=0.9))

# Arrow neuron â†’ activation
ax.annotate("", xy=(act_x - 0.7, act_y), xytext=(neuron_x + 0.8, neuron_y),
            arrowprops=dict(arrowstyle="->", color="purple", lw=2.5))
ax.text(6.75, 4.4, f"z = {z:.3f}", ha="center", fontsize=10,
        color="purple", fontweight="bold")

# Output arrow
ax.annotate("", xy=(9.8, act_y), xytext=(act_x + 0.7, act_y),
            arrowprops=dict(arrowstyle="->", color="#E91E63", lw=2.5))
output_val = sigmoid(np.array([z]))[0]
ax.text(9.5, 4.5, f"Å· = {output_val:.3f}\n({output_val*100:.0f}% rain)",
        ha="center", fontsize=10, fontweight="bold",
        bbox=dict(boxstyle="round", facecolor="#FCE4EC", edgecolor="#E91E63"))

plt.tight_layout()
plt.savefig(os.path.join(VIS_DIR, "neuron_diagram.png"), dpi=300, bbox_inches="tight")
plt.close()
print("   âœ… Saved: neuron_diagram.png")


# --- PLOT 3: Derivatives comparison ---
print("ğŸ“Š Generating: Activation function derivatives...")

fig, axes = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle("ğŸ“‰ Activation Derivatives â€” The Vanishing Gradient Problem",
             fontsize=14, fontweight="bold")

# Sigmoid + derivative
ax = axes[0]
ax.plot(x_range, sigmoid(x_range), "darkorange", linewidth=2, label="Sigmoid")
ax.plot(x_range, sigmoid_deriv, "red", linewidth=2, linestyle="--", label="Derivative")
ax.fill_between(x_range, sigmoid_deriv, alpha=0.2, color="red")
ax.set_title("Sigmoid: max derivative = 0.25\nâš ï¸ Vanishing gradient at extremes",
             fontsize=11, color="darkred")
ax.legend(fontsize=10)
ax.set_xlim(-5, 5); ax.grid(True, alpha=0.3)
ax.set_xlabel("z")

# Tanh + derivative
ax = axes[1]
ax.plot(x_range, tanh_fn(x_range), "purple", linewidth=2, label="Tanh")
ax.plot(x_range, tanh_deriv, "magenta", linewidth=2, linestyle="--", label="Derivative")
ax.fill_between(x_range, tanh_deriv, alpha=0.2, color="magenta")
ax.set_title("Tanh: max derivative = 1.0\nâš ï¸ Still vanishes at extremes",
             fontsize=11, color="purple")
ax.legend(fontsize=10)
ax.set_xlim(-5, 5); ax.grid(True, alpha=0.3)
ax.set_xlabel("z")

# ReLU + derivative
ax = axes[2]
ax.plot(x_range, relu(x_range), "green", linewidth=2, label="ReLU")
ax.plot(x_range, relu_deriv, "limegreen", linewidth=2, linestyle="--", label="Derivative")
ax.set_title("ReLU: derivative = 1 for z>0\nâœ… No vanishing gradient!",
             fontsize=11, color="darkgreen")
ax.legend(fontsize=10)
ax.set_xlim(-5, 5); ax.grid(True, alpha=0.3)
ax.set_xlabel("z")

plt.tight_layout()
plt.savefig(os.path.join(VIS_DIR, "activation_derivatives.png"), dpi=300, bbox_inches="tight")
plt.close()
print("   âœ… Saved: activation_derivatives.png")


# ======================================================================
# SECTION 7: QUICK REFERENCE SUMMARY
# ======================================================================
print()
print("=" * 70)
print("SECTION 7: QUICK REFERENCE â€” WHICH ACTIVATION TO USE?")
print("=" * 70)
print()
print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print("â”‚ Activation  â”‚ Output Range â”‚ When to Use                         â”‚")
print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
print("â”‚ ReLU        â”‚ [0, âˆ)       â”‚ Hidden layers (DEFAULT choice)      â”‚")
print("â”‚ Leaky ReLU  â”‚ (-âˆ, âˆ)      â”‚ Hidden layers (dying ReLU fix)      â”‚")
print("â”‚ Sigmoid     â”‚ (0, 1)       â”‚ Binary classification OUTPUT only   â”‚")
print("â”‚ Softmax     â”‚ (0,1) sums=1 â”‚ Multi-class classification OUTPUT   â”‚")
print("â”‚ Tanh        â”‚ (-1, 1)      â”‚ Hidden layers (zero-centered)       â”‚")
print("â”‚ Linear      â”‚ (-âˆ, âˆ)      â”‚ Regression OUTPUT only              â”‚")
print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
print()

print("=" * 70)
print("âœ… MODULE 1 COMPLETE!")
print("=" * 70)
print()
print("Key Takeaways:")
print("  ğŸ§  A neuron = weighted sum + bias + activation function")
print("  ğŸ”‘ Activation functions enable non-linear learning")
print("  âš ï¸  Sigmoid/Tanh suffer from vanishing gradients in deep networks")
print("  âœ… ReLU is the default choice for hidden layers")
print("  ğŸ“Š Softmax for multi-class output, Sigmoid for binary output")
print()
print("Next: Module 2 â†’ Forward Propagation (connecting neurons into layers!)")
print()
print(f"Visualizations saved to: {VIS_DIR}/")
print("  â€¢ activation_functions.png")
print("  â€¢ neuron_diagram.png")
print("  â€¢ activation_derivatives.png")


if __name__ == "__main__":
    pass
