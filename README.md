# ğŸ¤– Machine Learning for Beginners â€” Zero to LLMs

A comprehensive, **visual-first** ML course that teaches from absolute basics (algebra) to building Large Language Models from scratch. No ML or math background required.

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Parts](https://img.shields.io/badge/Parts-6%2F7%20Complete-success)]()
[![Visualizations](https://img.shields.io/badge/Visualizations-150%2B-orange)]()

---

## ğŸŒŸ What Makes This Course Special

| Feature | Detail |
|---------|--------|
| **Visual-First** | 150+ auto-generated PNG visualizations â€” every concept illustrated |
| **ğŸ—ºï¸ Mind Maps** | Per-module and full-course mind maps for big-picture understanding |
| **From Scratch** | Every algorithm built in pure NumPy before showing the library version |
| **No Prereqs** | Starts with `y = mx + b` and builds to Transformers & LLMs |
| **7 Complete Parts** | Regression â†’ Classification â†’ DNNs â†’ CNNs â†’ NLP â†’ Transformers â†’ LLMs |
| **YouTube Integration** | Curated StatQuest / 3Blue1Brown / Karpathy videos throughout |
| **Project-Based** | 15 real capstone projects across all parts |

---

## ğŸ—ºï¸ Course Mind Maps

Visual overviews are in `mind_maps/` â€” generated by `mind_maps/generate_mindmaps.py`.

```bash
python3 mind_maps/generate_mindmaps.py   # regenerate all mind maps
open mind_maps/course_overview.png       # full course big picture
open mind_maps/part6_transformers.png    # Part 6 detail
open mind_maps/modules/                  # per-concept mind maps
```

| File | Shows |
|------|-------|
| `course_overview.png` | All 7 parts radially connected |
| `part1_regression.png` â€¦ `part7_llms.png` | Per-part module hierarchy |
| `modules/attention_mechanism.png` | Attention Q/K/V, causal mask, Bahdanau vs Vaswani |
| `modules/transformer_architecture.png` | Encoder, decoder, residuals, FFN |
| `modules/bert_vs_gpt.png` | Bidirectional vs causal, masked LM vs next-token |
| `modules/neural_network_concepts.png` | Neurons, backprop, optimizers, regularization |
| `modules/cnn_concepts.png` | Convolution, pooling, receptive field |
| `modules/nlp_concepts.png` | Tokenization, TF-IDF, embeddings, LSTM |

---

## ğŸ“š Course Structure

### ğŸ”¹ Part 1: Regression âœ… 100% Complete

**Predict continuous values** â€” prices, temperatures, sales.

```
math_foundations/    01_algebra_basics Â· 02_statistics_fundamentals Â· 03_intro_to_derivatives
                     04_linear_algebra_basics Â· 05_probability_basics
algorithms/          linear_regression_intro Â· multiple_regression
examples/            simple_examples Â· data_exploration Â· model_evaluation
projects/            housing_analysis Â· house_price_prediction
```

**Progress: 12/12 modules** ğŸ‰

---

### ğŸ”¹ Part 2: Classification âœ… 100% Complete

**Predict categories** â€” spam/not spam, churn/no churn.

```
math_foundations/    01_sigmoid_function Â· 02_probability_for_classification Â· 03_log_loss
                     04_confusion_matrix Â· 05_decision_boundaries
algorithms/          logistic_regression_intro Â· knn_classifier Â· decision_trees
                     random_forests Â· metrics_deep_dive
projects/            spam_classifier Â· churn_prediction Â· model_comparison
```

**Progress: 16/16 modules** ğŸ‰

---

### ğŸ”¹ Part 3: Deep Neural Networks âœ… 100% Complete

**Build neural networks from scratch** â€” from a single neuron to full MLPs.

```
math_foundations/    01_neurons_and_activations Â· 02_forward_propagation Â· 03_backpropagation
                     04_loss_functions_and_optimizers Â· 05_regularization
algorithms/          perceptron_from_scratch Â· multilayer_perceptron Â· mlp_with_keras
                     hyperparameter_tuning
projects/            mnist_digit_classifier Â· tabular_deep_learning
visuals/             28 PNGs â€” activation functions, backprop flow, loss landscapes
```

**Progress: 11/11 modules** ğŸ‰

```bash
cd deep_neural_networks/math_foundations && python3 01_neurons_and_activations.py
cd ../algorithms && python3 multilayer_perceptron.py
cd ../projects && python3 mnist_digit_classifier.py
```

---

### ğŸ”¹ Part 4: Convolutional Neural Networks âœ… 100% Complete

**Vision and image understanding** â€” from pixels to ResNet.

```
math_foundations/    01_image_basics Â· 02_convolution_operation Â· 03_pooling_and_depth
algorithms/          conv_layer_from_scratch Â· cnn_with_keras Â· classic_architectures
                     transfer_learning
projects/            cifar10_classifier Â· custom_image_classifier
visuals/             20 PNGs â€” convolution animations, feature maps, architecture diagrams
```

**Progress: 9/9 modules** ğŸ‰

```bash
cd convolutional_neural_networks/math_foundations && python3 02_convolution_operation.py
cd ../algorithms && python3 conv_layer_from_scratch.py
cd ../projects && python3 cifar10_classifier.py
```

---

### ğŸ”¹ Part 5: Natural Language Processing âœ… 100% Complete

**Text understanding** â€” from tokenization to LSTMs.

```
math_foundations/    01_text_processing Â· 02_bag_of_words_tfidf Â· 03_word_embeddings
                     04_rnn_intuition
algorithms/          text_classification_pipeline Â· sentiment_analysis Â· lstm_text_classifier
                     named_entity_recognition
projects/            movie_review_sentiment Â· news_article_classifier
visuals/             30 PNGs â€” word clouds, embedding spaces, RNN diagrams, NER visualizations
```

**Progress: 10/10 modules** ğŸ‰  |  ğŸ“– [NLP README](nlp/README.md)

```bash
cd nlp/math_foundations && python3 03_word_embeddings.py
cd ../algorithms && python3 sentiment_analysis.py
cd ../projects && python3 movie_review_sentiment.py
```

---

### ğŸ”¹ Part 6: Transformers âœ… 100% Complete

**Attention is all you need** â€” from scaled dot-product to GPT-2.

```
math_foundations/    01_attention_mechanism Â· 02_multi_head_attention Â· 03_positional_encoding
                     04_encoder_decoder_arch
algorithms/          transformer_from_scratch Â· bert_encoder Â· gpt_decoder
projects/            bert_text_classifier Â· gpt2_text_generator
visuals/             27 PNGs â€” attention heatmaps, PE visualizations, BERT/GPT architecture diagrams
```

**Progress: 9/9 modules** ğŸ‰  |  ğŸ“– [Transformers README](transformers/README.md)

```bash
cd transformers/math_foundations && python3 01_attention_mechanism.py
cd ../algorithms && python3 transformer_from_scratch.py
cd ../projects && python3 bert_text_classifier.py
```

---

### ğŸ”¹ Part 7: Large Language Models ğŸ”¨ In Progress

**From tokens to trillion-parameter models** â€” build an LLM from scratch.

```
math_foundations/    01_how_llms_work Â· 02_prompt_engineering Â· 03_finetuning_basics
                     04_retrieval_augmented_generation
algorithms/          llm_from_scratch Â· lora_finetuning Â· rag_pipeline
projects/            qa_system_with_rag Â· llm_powered_classifier
```

```bash
cd llms/math_foundations && python3 01_how_llms_work.py
cd ../algorithms && python3 llm_from_scratch.py      # build mini-GPT in NumPy!
cd ../projects && python3 qa_system_with_rag.py
```

---

## ğŸš€ Quick Start

```bash
# 1. Clone
git clone https://github.com/ravithati-cse/machinelearningcourse.git
cd machinelearningcourse

# 2. Install base dependencies
pip install numpy pandas matplotlib seaborn scikit-learn

# 3. Install deep learning (Parts 3-7)
pip install tensorflow

# 4. Install NLP & Transformers (Parts 5-7)
pip install transformers torch spacy nltk

# 5. Run your first module
python3 regression_algorithms/math_foundations/01_algebra_basics.py

# 6. View generated visualizations
open regression_algorithms/visuals/

# 7. See the course mind map
python3 mind_maps/generate_mindmaps.py
open mind_maps/course_overview.png
```

---

## ğŸ¯ Learning Paths

### Path 1: Complete Beginner (Recommended â€” ~150 hrs)
Follow parts 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ 6 â†’ 7 in order. Every module builds on the last.

### Path 2: Already Know Math (~100 hrs)
Skip math_foundations in Parts 1-2. Start from `algorithms/` in each part.

### Path 3: Deep Learning Focus (~60 hrs)
Parts 3 â†’ 4 â†’ 5 â†’ 6 â†’ 7. Brief review of Parts 1-2 algorithms only.

### Path 4: Transformers & LLMs Only (~40 hrs)
Parts 6 â†’ 7. Assumes familiarity with neural networks.

---

## ğŸ“Š Course Statistics

```
Total Parts:          7  (6 complete, 1 in progress)
Total Modules:       67  (57 complete + 9 LLMs)
Lines of Code:   ~60,000+
Visualizations:    150+  auto-generated PNG files (300 dpi)
Mind Maps:          14   (1 course overview + 7 per-part + 6 per-concept)
Capstone Projects:  15   (2 per part Ã— 7 parts, plus 3 in Part 2)
YouTube Links:      70+  curated videos embedded throughout
ML Algorithms:      25+  implemented from scratch in NumPy
Learning Hours:   150+   total for complete path
```

---

## ğŸ—ºï¸ Visual Gallery

Every module generates 3 publication-quality PNG visualizations (300 dpi) automatically when run.

| Part | Visuals | Sample Topics |
|------|---------|--------------|
| Part 3: DNNs | 28 PNGs | Activation functions, backprop flow, loss landscapes, MNIST predictions |
| Part 4: CNNs | 20 PNGs | Convolution animations, feature maps, architecture comparisons, receptive fields |
| Part 5: NLP | 30 PNGs | Word clouds, embedding PCA, RNN diagrams, NER entity highlighting |
| Part 6: Transformers | 27 PNGs | Attention heatmaps, positional encoding, BERT/GPT architectures, embedding spaces |
| Part 7: LLMs | 27 PNGs | Training curves, LoRA diagrams, RAG pipelines, sampling strategy comparisons |
| Mind Maps | 14 PNGs | Course overview, per-part hierarchies, per-concept deep dives |

---

## ğŸ› ï¸ Tech Stack

| Layer | Tools |
|-------|-------|
| Core | Python 3.8+, NumPy, Pandas |
| Visualization | Matplotlib, Seaborn |
| Classical ML | Scikit-learn |
| Deep Learning | TensorFlow/Keras |
| NLP | spaCy, NLTK, HuggingFace Transformers |
| LLMs | HuggingFace `transformers`, `peft` (LoRA) |
| Interactive | Jupyter, ipywidgets |

All deep learning and NLP libraries are **optional** â€” every module degrades gracefully with a helpful install message if a library is missing.

---

## ğŸ“‚ Repository Structure

```
MLForBeginners/
â”œâ”€â”€ README.md                           â† This file
â”œâ”€â”€ mind_maps/                          â† ğŸ—ºï¸ 14 mind map PNGs + generator script
â”‚   â”œâ”€â”€ generate_mindmaps.py
â”‚   â”œâ”€â”€ course_overview.png
â”‚   â”œâ”€â”€ part1_regression.png â€¦ part7_llms.png
â”‚   â””â”€â”€ modules/                        â† Per-concept mind maps
â”‚
â”œâ”€â”€ regression_algorithms/              â† Part 1 âœ… (12 modules)
â”‚   â”œâ”€â”€ math_foundations/   algorithms/   projects/   visuals/
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ classification_algorithms/          â† Part 2 âœ… (16 modules)
â”‚   â”œâ”€â”€ math_foundations/   algorithms/   projects/   visuals/
â”‚
â”œâ”€â”€ deep_neural_networks/               â† Part 3 âœ… (11 modules, 28 visuals)
â”‚   â”œâ”€â”€ math_foundations/   algorithms/   projects/   visuals/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ convolutional_neural_networks/      â† Part 4 âœ… (9 modules, 20 visuals)
â”‚   â”œâ”€â”€ math_foundations/   algorithms/   projects/   visuals/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ nlp/                                â† Part 5 âœ… (10 modules, 30 visuals)
â”‚   â”œâ”€â”€ math_foundations/   algorithms/   projects/   visuals/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ transformers/                       â† Part 6 âœ… (9 modules, 27 visuals)
â”‚   â”œâ”€â”€ math_foundations/   algorithms/   projects/   visuals/
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ llms/                               â† Part 7 ğŸ”¨ (9 modules, 27 visuals)
    â”œâ”€â”€ math_foundations/   algorithms/   projects/   visuals/
    â””â”€â”€ README.md
```

---

## ğŸ“ What You'll Learn

### Parts 1â€“2: Classical ML
- âœ… Linear & logistic regression from scratch (gradient descent)
- âœ… Decision trees, random forests, KNN
- âœ… All evaluation metrics: accuracy, precision, recall, F1, AUC-ROC

### Part 3: Deep Neural Networks
- âœ… Neurons, activation functions (ReLU, sigmoid, tanh, GELU)
- âœ… Forward propagation â€” exact matrix math
- âœ… Backpropagation â€” chain rule step by step
- âœ… Optimizers: SGD, momentum, Adam
- âœ… Regularization: L1, L2, dropout, batch norm

### Part 4: CNNs
- âœ… Convolution operation from scratch
- âœ… Max/average pooling, stride, padding
- âœ… Classic architectures: LeNet, AlexNet, VGG, ResNet (skip connections)
- âœ… Transfer learning: freeze pretrained â†’ fine-tune head

### Part 5: NLP
- âœ… Tokenization, stemming, lemmatization
- âœ… TF-IDF, bag-of-words, n-grams
- âœ… Word2Vec / GloVe style embeddings
- âœ… RNNs, LSTMs, bidirectional RNNs
- âœ… Named entity recognition (NER)

### Part 6: Transformers
- âœ… Scaled dot-product attention: `Attention(Q,K,V) = softmax(QKáµ€/âˆšd_k)V`
- âœ… Multi-head attention, positional encoding (sinusoidal + learned + RoPE)
- âœ… Encoder-decoder architecture, Pre-LN vs Post-LN
- âœ… BERT (bidirectional encoder) and GPT (causal decoder)
- âœ… Feature extraction vs fine-tuning strategy

### Part 7: LLMs (In Progress)
- ğŸ”¨ How LLMs work: next-token prediction, scaling laws, emergent abilities
- ğŸ”¨ Prompt engineering: zero-shot, few-shot, chain-of-thought
- ğŸ”¨ Fine-tuning: SFT, LoRA from scratch, RLHF overview
- ğŸ”¨ RAG: chunking, TF-IDF retrieval, augmented prompting
- ğŸ”¨ **Build a mini-GPT from scratch** and train it on a corpus
- ğŸ”¨ Production Q&A system with RAG

---

## ğŸ’¡ Learning Tips

1. **Run every module** â€” outputs and visualizations are the lesson, not just the code
2. **Follow the order** â€” each module builds on the previous
3. **Read the print statements** â€” they explain the math as it happens
4. **Open the visuals** â€” every concept has a 300 dpi visualization
5. **Check the mind maps** â€” get your bearings after each part
6. **Experiment** â€” change hyperparameters, break things, observe what happens
7. **Watch the YouTube links** â€” embedded throughout for visual explanations

---

## ğŸ¤ Contributing

Contributions welcome:
- ğŸ› **Bug reports** â€” open an issue
- ğŸ’¡ **Module ideas** â€” discussions tab
- ğŸ“ **Documentation** â€” improve explanations
- ğŸ¨ **Visualizations** â€” better plots
- ğŸŒ **Translations** â€” help make it global

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## ğŸ™ Acknowledgments

- **StatQuest** by Josh Starmer â€” best ML explanations on YouTube
- **3Blue1Brown** â€” beautiful visual mathematics
- **Andrej Karpathy** â€” nanoGPT inspiration for LLMs from scratch
- **Andrew Ng** â€” course structure inspiration
- **The Annotated Transformer** (Harvard NLP) â€” transformer implementation reference

---

## ğŸ“œ License

MIT License â€” see [LICENSE](LICENSE). Use freely for learning, teaching, or commercial work.

---

## ğŸ“§ Contact

**Created by:** Ravi Thati
**GitHub:** [@ravithati-cse](https://github.com/ravithati-cse)
**Repository:** [machinelearningcourse](https://github.com/ravithati-cse/machinelearningcourse)

---

## ğŸ—ºï¸ Roadmap

### âœ… Completed
- Parts 1â€“6 (57/57 modules, 100%)
- 150+ visualizations
- 14 mind maps

### ğŸ”¨ In Progress
- Part 7: LLMs (9 modules)
- Jupyter notebook versions

### ğŸ”œ Planned
- Video walkthroughs
- MLOps and deployment module
- Evaluation & benchmarking module
- Community project showcase

---

## â­ Star This Repository!

If this course helps you learn ML, please â­ star it! It helps others discover the resource.

---

*Built with â¤ï¸ for aspiring ML engineers and data scientists*
*Last updated: February 2026*
