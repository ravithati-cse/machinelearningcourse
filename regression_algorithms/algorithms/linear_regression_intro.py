"""
üìà LINEAR REGRESSION - The Foundation of Machine Learning

================================================================================
LEARNING OBJECTIVES
================================================================================
After completing this module, you will understand:
1. What linear regression is and when to use it
2. The regression equation: ≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx
3. Cost functions and Mean Squared Error (MSE)
4. How to find the best fit line (normal equation)
5. Making predictions with linear regression
6. Interpreting coefficients (what do Œ≤‚ÇÄ and Œ≤‚ÇÅ mean?)
7. Using scikit-learn for linear regression

This is where ALL the math comes together!

================================================================================
üì∫ RECOMMENDED VIDEOS (MUST WATCH!)
================================================================================
‚≠ê ABSOLUTE MUST WATCH:
   - StatQuest: "Linear Regression, Clearly Explained!!!"
     https://www.youtube.com/watch?v=nk2CQITm_eo
     (The BEST introduction to linear regression - watch this first!)

   - StatQuest: "Linear Models Pt.1 - Linear Regression"
     https://www.youtube.com/watch?v=PaFPbb66DxQ
     (Goes deeper into the math)

Also Highly Recommended:
   - 3Blue1Brown: "Neural Networks Chapter 2" (Gradient Descent)
     https://www.youtube.com/watch?v=IHZwWFHWa-w

   - Khan Academy: "Introduction to residuals and least squares"
     https://www.youtube.com/watch?v=yMgFHbjbAW8

================================================================================
OVERVIEW
================================================================================
Linear regression is the FOUNDATION of machine learning!

It answers the question: "What's the relationship between X and Y?"
- X = input/feature (e.g., house size)
- Y = output/target (e.g., house price)

We find the BEST LINE that describes this relationship.

All the math you learned comes together here:
- Algebra: y = mx + b (the line equation)
- Statistics: correlation, mean, variance (finding the best fit)
- Calculus: derivatives, minimizing error (gradient descent)
- Linear Algebra: vectors, dot products (making predictions)
- Probability: normal distribution (understanding residuals)

Let's build a complete linear regression model from scratch!
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import os
import warnings
warnings.filterwarnings('ignore')

# Setup visualization directory
VISUAL_DIR = '../visuals/regression/'
os.makedirs(VISUAL_DIR, exist_ok=True)

print("=" * 80)
print("üìà LINEAR REGRESSION - Making Predictions with Lines")
print("=" * 80)
print()

# ============================================================================
# SECTION 1: THE PROBLEM - FINDING RELATIONSHIPS IN DATA
# ============================================================================
print("\n" + "=" * 80)
print("SECTION 1: The Problem - Understanding Relationships")
print("=" * 80)
print()

print("THE FUNDAMENTAL QUESTION:")
print("-" * 70)
print("Given data about X (input) and Y (output), can we predict Y from X?")
print()
print("Examples:")
print("  ‚Ä¢ X = hours studied, Y = test score")
print("  ‚Ä¢ X = house size, Y = house price")
print("  ‚Ä¢ X = advertising spend, Y = sales")
print("  ‚Ä¢ X = temperature, Y = ice cream sales")
print()

print("EXAMPLE DATASET: House Prices")
print("-" * 70)

# Generate sample data
np.random.seed(42)
sizes = np.array([800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600])
# True relationship: price = 150 * size + 50000 + noise
prices = 150 * sizes + 50000 + np.random.normal(0, 15000, len(sizes))

print(f"{'House Size (sqft)':<20} {'Price ($)':<15}")
print("-" * 40)
for size, price in zip(sizes, prices):
    print(f"{size:<20} ${price:<14,.0f}")

print()
print("QUESTION: If a house is 1500 sqft, what price should we predict?")
print()
print("To answer this, we need to:")
print("  1. Find the RELATIONSHIP between size and price")
print("  2. Express it as an EQUATION")
print("  3. Use the equation to PREDICT new prices")
print()
print("This is exactly what linear regression does!")
print()

# ============================================================================
# SECTION 2: THE REGRESSION EQUATION
# ============================================================================
print("\n" + "=" * 80)
print("SECTION 2: The Regression Equation")
print("=" * 80)
print()

print("LINEAR REGRESSION EQUATION:")
print("-" * 70)
print("  ≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx")
print()
print("Where:")
print("  ‚Ä¢ ≈∑ (y-hat) = PREDICTED value")
print("  ‚Ä¢ x = INPUT (feature, independent variable)")
print("  ‚Ä¢ Œ≤‚ÇÄ (beta-zero) = INTERCEPT")
print("    ‚Üí Value of y when x = 0")
print("    ‚Üí Where the line crosses the y-axis")
print("  ‚Ä¢ Œ≤‚ÇÅ (beta-one) = SLOPE")
print("    ‚Üí How much y changes when x increases by 1")
print("    ‚Üí Rate of change")
print()

print("THIS IS THE SAME AS y = mx + b from algebra!")
print("  ‚Ä¢ Œ≤‚ÇÅ is the slope (m)")
print("  ‚Ä¢ Œ≤‚ÇÄ is the intercept (b)")
print("  ‚Ä¢ ≈∑ is our prediction")
print()

print("FOR OUR HOUSE PRICE EXAMPLE:")
print("-" * 70)
print("  ≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó size")
print()
print("If we find Œ≤‚ÇÄ = 50,000 and Œ≤‚ÇÅ = 150:")
print("  ≈∑ = 50,000 + 150 √ó size")
print()
print("INTERPRETATION:")
print(f"  ‚Ä¢ Œ≤‚ÇÄ = 50,000: Base price (even for 0 sqft - not realistic!)")
print(f"  ‚Ä¢ Œ≤‚ÇÅ = 150: Each additional sqft adds $150 to price")
print()
print("PREDICTION for 1500 sqft house:")
print("  ≈∑ = 50,000 + 150 √ó 1500")
print("  ≈∑ = 50,000 + 225,000")
print("  ≈∑ = $275,000")
print()

# ============================================================================
# SECTION 3: COST FUNCTION - MEASURING ERROR
# ============================================================================
print("\n" + "=" * 80)
print("SECTION 3: The Cost Function - How Wrong Are We?")
print("=" * 80)
print()

print("THE PROBLEM:")
print("-" * 70)
print("There are INFINITE possible lines (infinite choices of Œ≤‚ÇÄ and Œ≤‚ÇÅ)")
print("We need to find the BEST line!")
print()
print("How do we measure 'best'?")
print("‚Üí The line that makes the SMALLEST ERRORS!")
print()

print("RESIDUALS (ERRORS):")
print("-" * 70)
print("For each data point:")
print("  ‚Ä¢ y·µ¢ = actual value (what we observed)")
print("  ‚Ä¢ ≈∑·µ¢ = predicted value (what our line predicts)")
print("  ‚Ä¢ Residual = y·µ¢ - ≈∑·µ¢ (the error)")
print()

print("Example calculations:")
actual_sample = prices[:3]
sizes_sample = sizes[:3]

# Make simple predictions with Œ≤‚ÇÄ=50000, Œ≤‚ÇÅ=150
predicted_sample = 50000 + 150 * sizes_sample

print(f"{'Size':<10} {'Actual':<15} {'Predicted':<15} {'Residual':<15}")
print("-" * 60)
for size, actual, pred in zip(sizes_sample, actual_sample, predicted_sample):
    residual = actual - pred
    print(f"{size:<10} ${actual:<14,.0f} ${pred:<14,.0f} ${residual:<14,.0f}")

print()

print("MEAN SQUARED ERROR (MSE) - The Cost Function:")
print("-" * 70)
print("MSE = (1/n) √ó Œ£(y·µ¢ - ≈∑·µ¢)¬≤")
print()
print("In words:")
print("  1. For each point: calculate (actual - predicted)¬≤")
print("  2. Add up all the squared errors")
print("  3. Divide by the number of points (n)")
print()
print("Why SQUARE the errors?")
print("  ‚Ä¢ Makes all errors positive (can't cancel out)")
print("  ‚Ä¢ Penalizes BIG errors more (2¬≤ = 4, but 4¬≤ = 16!)")
print("  ‚Ä¢ Mathematically convenient for optimization")
print()

# Calculate MSE manually
residuals = actual_sample - predicted_sample
squared_errors = residuals ** 2
mse = np.mean(squared_errors)

print("Calculating MSE for our sample:")
print(f"  Squared errors: {np.array2string(squared_errors, precision=0)}")
print(f"  MSE = {mse:,.0f}")
print()

print("GOAL OF LINEAR REGRESSION:")
print("  Find Œ≤‚ÇÄ and Œ≤‚ÇÅ that MINIMIZE MSE!")
print("  ‚Üí Smallest average squared error")
print("  ‚Üí Best fit line!")
print()

# ============================================================================
# SECTION 4: FINDING THE BEST LINE - THE NORMAL EQUATION
# ============================================================================
print("\n" + "=" * 80)
print("SECTION 4: Finding the Best Fit Line")
print("=" * 80)
print()

print("TWO METHODS TO FIND BEST Œ≤‚ÇÄ AND Œ≤‚ÇÅ:")
print("-" * 70)
print("1. GRADIENT DESCENT (calculus - what you learned!)")
print("   ‚Ä¢ Start with random Œ≤‚ÇÄ, Œ≤‚ÇÅ")
print("   ‚Ä¢ Calculate derivatives")
print("   ‚Ä¢ Update: Œ≤ = Œ≤ - Œ± √ó derivative")
print("   ‚Ä¢ Repeat until MSE is minimized")
print()
print("2. NORMAL EQUATION (closed-form solution - we'll use this!)")
print("   ‚Ä¢ Direct mathematical formula")
print("   ‚Ä¢ Gives exact answer in one step")
print("   ‚Ä¢ Uses statistics: mean, covariance, variance")
print()

print("NORMAL EQUATION FORMULAS:")
print("-" * 70)
print("  Œ≤‚ÇÅ = Œ£((x·µ¢ - xÃÑ)(y·µ¢ - »≥)) / Œ£(x·µ¢ - xÃÑ)¬≤")
print()
print("  Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅxÃÑ")
print()
print("Where:")
print("  ‚Ä¢ xÃÑ = mean of x")
print("  ‚Ä¢ »≥ = mean of y")
print("  ‚Ä¢ The numerator of Œ≤‚ÇÅ is COVARIANCE!")
print("  ‚Ä¢ The denominator of Œ≤‚ÇÅ is VARIANCE!")
print()
print("Remember from statistics?")
print("  ‚Üí Œ≤‚ÇÅ = Cov(x, y) / Var(x)")
print("  ‚Üí This is how correlation and regression connect!")
print()

print("CALCULATING Œ≤‚ÇÅ AND Œ≤‚ÇÄ MANUALLY:")
print("-" * 70)

# Manual calculation
x_mean = np.mean(sizes)
y_mean = np.mean(prices)

print(f"Step 1: Calculate means")
print(f"  xÃÑ (mean size) = {x_mean:.1f} sqft")
print(f"  »≥ (mean price) = ${y_mean:,.0f}")
print()

# Calculate deviations
x_deviations = sizes - x_mean
y_deviations = prices - y_mean

print("Step 2: Calculate deviations from mean")
print(f"  (x·µ¢ - xÃÑ) for first few: {np.array2string(x_deviations[:3], precision=1)}")
print(f"  (y·µ¢ - »≥) for first few: {np.array2string(y_deviations[:3], precision=0)}")
print()

# Calculate Œ≤‚ÇÅ (slope)
numerator = np.sum(x_deviations * y_deviations)
denominator = np.sum(x_deviations ** 2)
beta_1 = numerator / denominator

print("Step 3: Calculate Œ≤‚ÇÅ (slope)")
print(f"  Numerator (covariance √ó n) = Œ£(x·µ¢-xÃÑ)(y·µ¢-»≥) = {numerator:,.0f}")
print(f"  Denominator (variance √ó n) = Œ£(x·µ¢-xÃÑ)¬≤ = {denominator:,.0f}")
print(f"  Œ≤‚ÇÅ = {numerator:,.0f} / {denominator:,.0f} = {beta_1:.2f}")
print()

# Calculate Œ≤‚ÇÄ (intercept)
beta_0 = y_mean - beta_1 * x_mean

print("Step 4: Calculate Œ≤‚ÇÄ (intercept)")
print(f"  Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅxÃÑ")
print(f"  Œ≤‚ÇÄ = {y_mean:,.0f} - {beta_1:.2f} √ó {x_mean:.1f}")
print(f"  Œ≤‚ÇÄ = {beta_0:,.0f}")
print()

print("‚úÖ OUR EQUATION:")
print(f"  ≈∑ = {beta_0:,.0f} + {beta_1:.2f} √ó size")
print()

print("INTERPRETATION:")
print(f"  ‚Ä¢ Base price (Œ≤‚ÇÄ): ${beta_0:,.0f}")
print(f"  ‚Ä¢ Price per sqft (Œ≤‚ÇÅ): ${beta_1:.2f}")
print(f"  ‚Ä¢ For each additional sqft, price increases by ${beta_1:.2f}")
print()

# ============================================================================
# SECTION 5: MAKING PREDICTIONS
# ============================================================================
print("\n" + "=" * 80)
print("SECTION 5: Making Predictions with Our Model")
print("=" * 80)
print()

print("Now that we have Œ≤‚ÇÄ and Œ≤‚ÇÅ, we can predict ANY house price!")
print()

# Make predictions
test_sizes = [1500, 1750, 2100]

print(f"{'House Size':<15} {'Prediction Calculation':<40} {'Predicted Price'}")
print("-" * 75)
for test_size in test_sizes:
    prediction = beta_0 + beta_1 * test_size
    calc_str = f"{beta_0:,.0f} + {beta_1:.2f} √ó {test_size}"
    print(f"{test_size} sqft{' ':<7} {calc_str:<40} ${prediction:,.0f}")

print()

print("HOW GOOD ARE OUR PREDICTIONS?")
print("-" * 70)

# Calculate MSE for our model
predictions = beta_0 + beta_1 * sizes
residuals = prices - predictions
mse = np.mean(residuals ** 2)
rmse = np.sqrt(mse)

print("Calculating error metrics:")
print(f"  MSE (Mean Squared Error) = ${mse:,.0f}")
print(f"  RMSE (Root MSE) = ${rmse:,.0f}")
print()
print("RMSE interpretation:")
print(f"  Our predictions are off by about ${rmse:,.0f} on average")
print()

# Calculate R¬≤
ss_total = np.sum((prices - y_mean) ** 2)
ss_residual = np.sum(residuals ** 2)
r_squared = 1 - (ss_residual / ss_total)

print("R¬≤ Score (Coefficient of Determination):")
print(f"  R¬≤ = {r_squared:.4f} ({r_squared*100:.2f}%)")
print()
print("R¬≤ interpretation:")
print(f"  Our model explains {r_squared*100:.2f}% of the variance in prices!")
print(f"  {(1-r_squared)*100:.2f}% is due to other factors or noise")
print()

# ============================================================================
# VISUALIZATION 1: The Regression Line
# ============================================================================
print("üìä Generating Visualization 1: The Best Fit Line...")

fig, axes = plt.subplots(2, 2, figsize=(14, 12))
fig.suptitle('üìà LINEAR REGRESSION: Finding the Best Fit Line',
             fontsize=16, fontweight='bold', y=0.995)

# Plot 1: Scatter plot with regression line
ax = axes[0, 0]

# Scatter plot of data
ax.scatter(sizes, prices, color='blue', s=100, alpha=0.6, edgecolor='black', label='Actual data', zorder=5)

# Plot regression line
x_line = np.linspace(sizes.min(), sizes.max(), 100)
y_line = beta_0 + beta_1 * x_line
ax.plot(x_line, y_line, 'r-', linewidth=3, label=f'≈∑ = {beta_0:,.0f} + {beta_1:.1f}x', zorder=3)

# Plot residuals as vertical lines
for size, price, pred in zip(sizes, prices, predictions):
    ax.plot([size, size], [price, pred], 'g--', linewidth=1, alpha=0.5)

ax.set_xlabel('House Size (sqft)', fontsize=11)
ax.set_ylabel('Price ($)', fontsize=11)
ax.set_title('Linear Regression: Best Fit Line', fontsize=12, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

# Annotate one point
sample_idx = 5
ax.annotate(f'Actual: ${prices[sample_idx]:,.0f}\nPredicted: ${predictions[sample_idx]:,.0f}\nError: ${residuals[sample_idx]:,.0f}',
            xy=(sizes[sample_idx], prices[sample_idx]),
            xytext=(sizes[sample_idx] - 300, prices[sample_idx] + 40000),
            fontsize=8,
            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8),
            arrowprops=dict(arrowstyle='->', color='black', lw=1.5))

# Plot 2: Residual plot
ax = axes[0, 1]

ax.scatter(predictions, residuals, color='purple', s=100, alpha=0.6, edgecolor='black')
ax.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero error line')

ax.set_xlabel('Predicted Price ($)', fontsize=11)
ax.set_ylabel('Residual ($)', fontsize=11)
ax.set_title('Residual Plot\n(Should look random!)', fontsize=12, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

ax.text(predictions.mean(), residuals.max() * 0.8,
        'Good: Points scattered randomly\n‚Üí No patterns\n‚Üí Model fits well',
        ha='center', fontsize=9, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

# Plot 3: Actual vs Predicted
ax = axes[1, 0]

ax.scatter(prices, predictions, color='orange', s=100, alpha=0.6, edgecolor='black', label='Our predictions')

# Perfect prediction line
min_val = min(prices.min(), predictions.min())
max_val = max(prices.max(), predictions.max())
ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect predictions')

ax.set_xlabel('Actual Price ($)', fontsize=11)
ax.set_ylabel('Predicted Price ($)', fontsize=11)
ax.set_title(f'Actual vs Predicted (R¬≤ = {r_squared:.3f})', fontsize=12, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

ax.text(prices.mean(), predictions.max(),
        'Points near red line ‚Üí Good predictions\nPoints far from line ‚Üí Poor predictions',
        ha='center', fontsize=9, bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))

# Plot 4: Model summary
ax = axes[1, 1]
ax.text(0.5, 0.95, 'LINEAR REGRESSION SUMMARY', fontsize=12, fontweight='bold',
        ha='center', transform=ax.transAxes)

summary = [
    "üìä EQUATION:",
    f"   ≈∑ = {beta_0:,.0f} + {beta_1:.2f} √ó size",
    "",
    "üìè COEFFICIENTS:",
    f"   Œ≤‚ÇÄ (intercept) = ${beta_0:,.0f}",
    f"   Œ≤‚ÇÅ (slope) = ${beta_1:.2f} per sqft",
    "",
    "üìà INTERPRETATION:",
    f"   ‚Ä¢ Base price: ${beta_0:,.0f}",
    f"   ‚Ä¢ Each sqft adds: ${beta_1:.2f}",
    "",
    "‚úÖ PERFORMANCE:",
    f"   ‚Ä¢ MSE = ${mse:,.0f}",
    f"   ‚Ä¢ RMSE = ${rmse:,.0f}",
    f"   ‚Ä¢ R¬≤ = {r_squared:.4f} ({r_squared*100:.1f}%)",
    "",
    "üéØ WHAT THIS MEANS:",
    f"   Our model explains {r_squared*100:.1f}% of",
    "   price variation!",
    f"   Typical error: ¬±${rmse:,.0f}",
    "",
    "üí° EXAMPLE PREDICTION:",
    "   For 1500 sqft house:",
    f"   ≈∑ = {beta_0:,.0f} + {beta_1:.2f}√ó1500",
    f"   ≈∑ = ${beta_0 + beta_1*1500:,.0f}"
]

y_pos = 0.87
for line in summary:
    if line.startswith(('üìä', 'üìè', 'üìà', '‚úÖ', 'üéØ', 'üí°')):
        weight = 'bold'
        size = 9.5
    else:
        weight = 'normal'
        size = 8.5
    ax.text(0.5, y_pos, line, fontsize=size, ha='center', transform=ax.transAxes,
            family='monospace', fontweight=weight)
    y_pos -= 0.036

ax.axis('off')

plt.tight_layout()
plt.savefig(f'{VISUAL_DIR}01_linear_regression_basics.png', dpi=300, bbox_inches='tight', facecolor='white')
plt.close()

print("‚úÖ Saved: 01_linear_regression_basics.png")
print()

# ============================================================================
# SECTION 6: USING SCIKIT-LEARN
# ============================================================================
print("\n" + "=" * 80)
print("SECTION 6: Linear Regression with Scikit-Learn")
print("=" * 80)
print()

print("We did it manually to understand the math!")
print("Now let's use scikit-learn - the standard ML library")
print()

print("SCIKIT-LEARN CODE:")
print("-" * 70)

# Reshape data for sklearn (needs 2D array)
X = sizes.reshape(-1, 1)  # Features (2D)
y = prices  # Target (1D)

print("Step 1: Prepare data")
print(f"  X shape: {X.shape} (10 samples, 1 feature)")
print(f"  y shape: {y.shape} (10 values)")
print()

# Create and fit model
model = LinearRegression()
model.fit(X, y)

print("Step 2: Create and train model")
print("  model = LinearRegression()")
print("  model.fit(X, y)")
print()

print("Step 3: Extract coefficients")
sklearn_beta_0 = model.intercept_
sklearn_beta_1 = model.coef_[0]

print(f"  Intercept (Œ≤‚ÇÄ) = ${sklearn_beta_0:,.2f}")
print(f"  Slope (Œ≤‚ÇÅ) = ${sklearn_beta_1:.2f}")
print()

print("COMPARING OUR MANUAL CALCULATION vs SCIKIT-LEARN:")
print("-" * 70)
print(f"{'Parameter':<20} {'Manual':<20} {'Scikit-Learn':<20} {'Match?'}")
print("-" * 75)
print(f"{'Œ≤‚ÇÄ (intercept)':<20} ${beta_0:<19,.2f} ${sklearn_beta_0:<19,.2f} {'‚úÖ' if abs(beta_0 - sklearn_beta_0) < 1 else '‚ùå'}")
print(f"{'Œ≤‚ÇÅ (slope)':<20} ${beta_1:<19,.2f} ${sklearn_beta_1:<19,.2f} {'‚úÖ' if abs(beta_1 - sklearn_beta_1) < 0.01 else '‚ùå'}")
print()
print("They match! Our manual calculation was correct! üéâ")
print()

# Make predictions with sklearn
sklearn_predictions = model.predict(X)

print("Step 4: Make predictions")
print("  predictions = model.predict(X)")
print()
print(f"{'Size':<10} {'Actual':<15} {'Predicted':<15} {'Error'}")
print("-" * 55)
for size, actual, pred in zip(sizes[:5], prices[:5], sklearn_predictions[:5]):
    error = actual - pred
    print(f"{size:<10} ${actual:<14,.0f} ${pred:<14,.0f} ${error:,.0f}")
print("...")
print()

# Evaluate
sklearn_mse = mean_squared_error(y, sklearn_predictions)
sklearn_r2 = r2_score(y, sklearn_predictions)

print("Step 5: Evaluate model")
print(f"  MSE = ${sklearn_mse:,.0f}")
print(f"  R¬≤ = {sklearn_r2:.4f}")
print()

# ============================================================================
# SECTION 7: ASSUMPTIONS AND WHEN TO USE LINEAR REGRESSION
# ============================================================================
print("\n" + "=" * 80)
print("SECTION 7: When to Use Linear Regression")
print("=" * 80)
print()

print("LINEAR REGRESSION WORKS BEST WHEN:")
print("-" * 70)
print("‚úÖ 1. LINEAR RELATIONSHIP:")
print("     The relationship between X and Y is approximately a straight line")
print()
print("‚úÖ 2. INDEPENDENCE:")
print("     Data points are independent (one doesn't affect another)")
print()
print("‚úÖ 3. NORMAL RESIDUALS:")
print("     Errors are normally distributed (bell curve)")
print()
print("‚úÖ 4. CONSTANT VARIANCE (Homoscedasticity):")
print("     Errors have similar spread across all values of X")
print()

print("WHEN NOT TO USE LINEAR REGRESSION:")
print("-" * 70)
print("‚ùå Non-linear relationship (curve, not line)")
print("‚ùå Categorical target (use classification instead)")
print("‚ùå Extreme outliers dominating the fit")
print("‚ùå Time series with trends/seasonality (need special methods)")
print()

print("HOW TO CHECK ASSUMPTIONS:")
print("-" * 70)
print("1. Plot scatter plot ‚Üí Should see roughly linear pattern")
print("2. Plot residuals ‚Üí Should look random, no patterns")
print("3. Plot histogram of residuals ‚Üí Should look normal (bell curve)")
print("4. Check for outliers ‚Üí Points far from the line")
print()

# ============================================================================
# VISUALIZATION 2: Complete Workflow
# ============================================================================
print("üìä Generating Visualization 2: Complete Workflow...")

fig = plt.figure(figsize=(16, 10))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
fig.suptitle('üìà COMPLETE LINEAR REGRESSION WORKFLOW',
             fontsize=16, fontweight='bold')

# Plot 1: Raw data
ax1 = fig.add_subplot(gs[0, 0])
ax1.scatter(sizes, prices, color='blue', s=80, alpha=0.6, edgecolor='black')
ax1.set_xlabel('Size (sqft)', fontsize=10)
ax1.set_ylabel('Price ($)', fontsize=10)
ax1.set_title('Step 1: Collect Data', fontsize=11, fontweight='bold')
ax1.grid(True, alpha=0.3)

# Plot 2: Find best line
ax2 = fig.add_subplot(gs[0, 1])
ax2.scatter(sizes, prices, color='blue', s=80, alpha=0.6, edgecolor='black')
ax2.plot(x_line, y_line, 'r-', linewidth=3)
ax2.set_xlabel('Size (sqft)', fontsize=10)
ax2.set_ylabel('Price ($)', fontsize=10)
ax2.set_title('Step 2: Fit Line (Minimize MSE)', fontsize=11, fontweight='bold')
ax2.grid(True, alpha=0.3)
ax2.text(sizes.mean(), prices.max(), f'≈∑ = {beta_0:,.0f} + {beta_1:.1f}x',
         ha='center', fontsize=9, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))

# Plot 3: Make predictions
ax3 = fig.add_subplot(gs[0, 2])
ax3.scatter(sizes, prices, color='blue', s=60, alpha=0.4, label='Training data')
ax3.plot(x_line, y_line, 'r-', linewidth=2, alpha=0.5)

# New points to predict
new_sizes = np.array([1300, 1900, 2300])
new_predictions = beta_0 + beta_1 * new_sizes
ax3.scatter(new_sizes, new_predictions, color='green', s=150, marker='*',
            edgecolor='black', linewidth=2, label='Predictions', zorder=5)

for ns, np_val in zip(new_sizes, new_predictions):
    ax3.annotate(f'${np_val:,.0f}', xy=(ns, np_val), xytext=(ns, np_val + 20000),
                fontsize=8, ha='center', fontweight='bold')

ax3.set_xlabel('Size (sqft)', fontsize=10)
ax3.set_ylabel('Price ($)', fontsize=10)
ax3.set_title('Step 3: Make Predictions', fontsize=11, fontweight='bold')
ax3.legend()
ax3.grid(True, alpha=0.3)

# Plot 4: Residuals histogram
ax4 = fig.add_subplot(gs[1, 0])
ax4.hist(residuals, bins=7, color='lightblue', edgecolor='black', alpha=0.7)
ax4.axvline(x=0, color='red', linestyle='--', linewidth=2)
ax4.set_xlabel('Residual ($)', fontsize=10)
ax4.set_ylabel('Frequency', fontsize=10)
ax4.set_title('Step 4a: Check Residuals\n(Should be normal)', fontsize=10, fontweight='bold')
ax4.grid(True, alpha=0.3, axis='y')

# Plot 5: Residuals vs fitted
ax5 = fig.add_subplot(gs[1, 1])
ax5.scatter(predictions, residuals, color='purple', s=80, alpha=0.6, edgecolor='black')
ax5.axhline(y=0, color='red', linestyle='--', linewidth=2)
ax5.set_xlabel('Fitted values ($)', fontsize=10)
ax5.set_ylabel('Residual ($)', fontsize=10)
ax5.set_title('Step 4b: Check Residuals\n(Should be random)', fontsize=10, fontweight='bold')
ax5.grid(True, alpha=0.3)

# Plot 6: Q-Q plot (normal check)
ax6 = fig.add_subplot(gs[1, 2])
from scipy import stats as sp_stats
sp_stats.probplot(residuals, dist="norm", plot=ax6)
ax6.set_title('Step 4c: Normal Q-Q Plot\n(Check normality)', fontsize=10, fontweight='bold')
ax6.grid(True, alpha=0.3)

# Plot 7: Code example
ax7 = fig.add_subplot(gs[2, :])
code_text = """
SCIKIT-LEARN CODE:

# 1. Import
from sklearn.linear_model import LinearRegression

# 2. Prepare data
X = sizes.reshape(-1, 1)  # 2D array
y = prices

# 3. Create and train
model = LinearRegression()
model.fit(X, y)

# 4. Get coefficients
Œ≤‚ÇÄ = model.intercept_
Œ≤‚ÇÅ = model.coef_[0]

# 5. Make predictions
predictions = model.predict(X)

# 6. Evaluate
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y, predictions)
r2 = r2_score(y, predictions)

That's it! Just 6 steps to build a complete linear regression model!
"""

ax7.text(0.05, 0.95, code_text, transform=ax7.transAxes,
         fontsize=9, family='monospace', verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
ax7.axis('off')

plt.savefig(f'{VISUAL_DIR}02_complete_workflow.png', dpi=300, bbox_inches='tight', facecolor='white')
plt.close()

print("‚úÖ Saved: 02_complete_workflow.png")
print()

# ============================================================================
# FINAL SUMMARY
# ============================================================================
print("\n" + "=" * 80)
print("‚úÖ SUMMARY: Linear Regression Complete!")
print("=" * 80)
print()

print("üéØ WHAT WE LEARNED:")
print("-" * 70)
print("1. LINEAR REGRESSION EQUATION:")
print("   ≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx")
print("   ‚Ä¢ Œ≤‚ÇÄ = intercept (base value)")
print("   ‚Ä¢ Œ≤‚ÇÅ = slope (rate of change)")
print()

print("2. COST FUNCTION (MSE):")
print("   MSE = (1/n) √ó Œ£(y·µ¢ - ≈∑·µ¢)¬≤")
print("   ‚Ä¢ Measures average squared error")
print("   ‚Ä¢ Goal: minimize MSE")
print()

print("3. FINDING BEST FIT LINE:")
print("   ‚Ä¢ Normal equation (closed-form)")
print("   ‚Ä¢ Or gradient descent (iterative)")
print("   ‚Ä¢ Both find Œ≤‚ÇÄ and Œ≤‚ÇÅ that minimize MSE")
print()

print("4. MAKING PREDICTIONS:")
print("   ‚Ä¢ Plug in x value")
print("   ‚Ä¢ Calculate ≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx")
print("   ‚Ä¢ That's your prediction!")
print()

print("5. EVALUATING MODEL:")
print("   ‚Ä¢ MSE: average squared error")
print("   ‚Ä¢ RMSE: ‚àöMSE (same units as y)")
print("   ‚Ä¢ R¬≤: % of variance explained")
print()

print("ü§ñ USING SCIKIT-LEARN:")
print("-" * 70)
print("  from sklearn.linear_model import LinearRegression")
print("  model = LinearRegression()")
print("  model.fit(X, y)")
print("  predictions = model.predict(X)")
print()

print("üìä KEY INSIGHTS:")
print("-" * 70)
print("  ‚Ä¢ Linear regression finds the best straight line through data")
print("  ‚Ä¢ 'Best' means minimizing prediction errors (MSE)")
print("  ‚Ä¢ Coefficients have clear interpretations")
print("  ‚Ä¢ Works great when relationship is linear!")
print()

print("=" * 80)
print("üìÅ Visualizations saved to:", VISUAL_DIR)
print("=" * 80)
print("‚úÖ 01_linear_regression_basics.png")
print("‚úÖ 02_complete_workflow.png")
print("=" * 80)
print()

print("üéì NEXT STEPS:")
print("   1. Review visualizations - understand the complete workflow")
print("   2. Watch StatQuest video on linear regression (absolute must!)")
print("   3. Try with your own data - change the house sizes and prices!")
print("   4. Next: algorithms/multiple_regression.py (multiple features)")
print()

print("üí° REMEMBER:")
print("   All of machine learning builds on this foundation!")
print("   ‚Ä¢ Neural networks = stacked linear regressions (+ nonlinearity)")
print("   ‚Ä¢ Decision trees = piece-wise linear regressions")
print("   ‚Ä¢ Everything connects back to finding patterns in data!")
print()

print("=" * 80)
print("üéâ LINEAR REGRESSION MASTERED!")
print("   You now understand how ML models learn from data!")
print("=" * 80)
